# 显存估算(以qwen2.5-3B-Instruct为例)
在FP16/BF16全量微调情况下需要60~70GB左右显存, LoRA微调需要30~40GB左右显存. 显存占用来自3部分, 参考模型部署和推理, 目标模型旧策略部署和推理, 目标模型新策略训练.

本项目中参考模型和目标模型都用Qwen2.5-3B-Instruct, 参数类型为BF16, 参考模型、目标模型旧策略的部署和推理约占20G左右显存. 目标模型新策略训练占用情况如下表所示

全量微调显存占用分析表
| 配置                                     | 显存占用 |
|-----------------------------------------|---------|
| 模型参数(1份模型参数)                      | 6GB     |
| 梯度(1份模型参数)                         | 6GB     |
| AdamW优化器(F32 2份模型参数, 1阶矩+2阶矩)   | 24GB    |
| 激活值等                                 | 5-15GB  |
| 总计                                    | 41-51GB  |

LoRA微调显存显存占用分析表
| 配置                                       |   显存占用    |
|--------------------------------------------|-------------|
| 模型参数(1份模型参数+1份旁路矩阵参数)           | 6GB+0.2GB   |
| 梯度(1份旁路矩阵参数)                        | 0.2GB       |
| AdamW优化器(F32 2份旁路矩阵参数, 1阶矩+2阶矩)  | 0.75GB      |
| 激活值等                                    | 5-15GB      |
| 总计                                       | 12-22GB     |

LoRA旁路矩阵的参数量估算方式:
$$
n_{LoRA} = n_{total} \frac{2r}{d_{model}}
$$
n_LoRA表示LoRA的旁路矩阵参数量, n_total表示模型的总参数量, r表示秩(旁路矩阵的维度), d_model表示模型的隐藏层维度.
对于Qwen2.5-3B模型来说d_model=2048, 以r=32为例, 涉及到训练参数, 梯度, 优化器状态的旁路矩阵参数量为全量微调的64/2048=1/32